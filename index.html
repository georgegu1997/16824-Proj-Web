<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Qiao Gu Andrew ID: qiaog" />
  <meta name="author" content="Yanjia Duan Andrew ID: yanjiad" />
  <title>Towards Point Cloud Rotation Invariance using Point Pair Features</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href=".\pandoc.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Towards Point Cloud Rotation Invariance using Point Pair Features</h1>
<p class="author">Qiao Gu<br />
Andrew ID: qiaog</p>
<p class="author">Yanjia Duan<br />
Andrew ID: yanjiad</p>
</header>

<div style="text-align: center;">
  <iframe width="1024" height="768" src="https://www.youtube.com/embed/Wkdv5R0nSGg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>


<h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Deep learning on 3D point cloud has been widely studied by the computer vision community over the past few years <span class="citation" data-cites="Qi2017pointnet qi2017pointnet2 Wang2019-dgcnn Li2018-PointCNN">[1]–[4]</span>. However, a recent benchmark <span class="citation" data-cites="taghanaki2020robustpointset">[5]</span> demonstrates that most of these network structures are not rotation invariant and their performance drops dramatically if no rotation augmentation is applied during training while the point cloud is randomly rotated at test time. We believe that this is because in most deep neural networks for point cloud, the input is Cartesian coordinates of each point. PointNet <span class="citation" data-cites="Qi2017pointnet">[1]</span>, as one of the most basic architectures, projects the point coordinates into a high-dimensional space using per-point MLP, then uses a max-pooling layer to extract occupancy in the high-dimensional space, which maps back to complex geometric surfaces in the original space. This essentially gives a description of the shape of the point cloud, which could be used in point cloud classification and segmentation tasks. However, taking raw coordinates as input requires the input point cloud to be aligned with a “canonical” pose, and this necessitates the input 6-DoF transformation proposed in <span class="citation" data-cites="Qi2017pointnet">[1]</span>.</p>
<p>Therefore, in the project, we explore the usage of Point Pair Features (PPF), a rotation-invariant representation that has been widely used for 6D pose estimation and the point cloud registration task in the context of deep neural networks. We specifically focus on improving the rotation robustness of these networks by incorporating the special invariant property of PPF. We adapt the widely-used PointNet <span class="citation" data-cites="Qi2017pointnet">[1]</span>, PointNet<code>++</code> <span class="citation" data-cites="qi2017pointnet2">[2]</span> and Dynamic Graph CNN (DGCNN) <span class="citation" data-cites="Wang2019-dgcnn">[3]</span> structures and change their input to per-point PPF featurizaion instead of pure 3D point coordinates. We design several simple sampling strategies to select the reference point and convert each point in the point cloud to the 4-dimensional PPF vector. We demonstrate that incorporating PPF features can greatly improve the rotation robustness in the absence of training-time rotation augmentation. However, when applying training time augmentation used in <span class="citation" data-cites="qi2017pointnet2">[2]</span>, we found that PPF featurization leads to unstable performance. We hypothesize that this is because the straightforward sampling strategies we adopt are not robust to point cloud perturbation. Moreover, taking the inspiration from PointNet<code>++</code> <span class="citation" data-cites="qi2017pointnet2">[2]</span>, we incorporate the PPF featurization in the first point set abstraction layer and use sampled group centers as reference points, which improved the test time classification accuracy by 1.84%.</p>
<h1 data-number="2" id="related-work"><span class="header-section-number">2</span> Related Work</h1>
<h2 data-number="2.1" id="d-perception"><span class="header-section-number">2.1</span> 3D Perception</h2>
<p>3D perception has been extensively studied by the deep learning community, and point cloud is one of the most prevalent data formats for 3D representation. Numerous network architectures <span class="citation" data-cites="Qi2017pointnet qi2017pointnet2 Wang2019-dgcnn Li2018-PointCNN Wu2019-pointconv Liu2019-densepoint">[1]–[4], [6], [7]</span> have been proposed over the past few years to learn feature representations useful for the down-streaming tasks (i.e. classification, segmentation, generation, etc.). While these methods tackled the challenges brought by point cloud irregularity, unstructuredness, and unorderedness, a recent work <span class="citation" data-cites="taghanaki2020robustpointset">[5]</span> showed that they are not robust to different types of point cloud perturbation (noise, translation, occlusion, etc) and will fail significantly when test-time rotation perturbation is applied. Most methods only achieved around 10% classification accuracy on rotated ModelNet <span class="citation" data-cites="Wu2015-3dshapenet">[8]</span> testing set. SPHnet <span class="citation" data-cites="Poulenard2019-sphnet">[9]</span> and PRIN <span class="citation" data-cites="You2021-prin">[10]</span> show their invariance to rotated point clouds by specially-designed rotation-invariant local features and network structures, but their basic performance on point cloud classification is reported to be much lower than other networks.</p>
<h2 data-number="2.2" id="point-pair-features"><span class="header-section-number">2.2</span> Point Pair Features</h2>
<p>Point Pair Feature (PPF), first proposed by Drost. <em>et al.</em> <span class="citation" data-cites="Drost2010-ppf">[11]</span>, is a four-dimensional feature describing the relative relationship between two points in the same point cloud. Determined by its design, it is completely invariant to any 6D transformation, including rotation and translation. Even without any deep learning components, PPF has shown its great power in 6D pose estimation <span class="citation" data-cites="Eunyoung_Kim2011-3dobject Birdal2015-ppfbased Hinterstoisser2016-goingfurther Kiforenko2018-ppfeval">[12]–[15]</span>, and one variant of it <span class="citation" data-cites="Vidal2018-rh">[16]</span> is still among the state-of-the-art methods in BOP Challenge <span class="citation" data-cites="Hodan2018-bop">[17]</span>. Some recent works <span class="citation" data-cites="Deng2018-PPFNet Deng2018-PPFFoldNet">[18], [19]</span> exploit the power of PPF in designing 3D local descriptors and show great performance in point cloud registration. But the power of PPF is still relatively under-explored in the deep learning context, and to our best knowledge, no work has integrated PPF into deep neural networks for point cloud classification or segmentation.</p>
<h1 data-number="3" id="method"><span class="header-section-number">3</span> Method</h1>
<p>We designed two strategies for PPF featurization, which are (1) input-level featurization, (2) anchor point-based featurization. The input-level featurization is performed on PointNet, PointNet<code>++</code> and DGCNN. Anchor point-based featurization is performed on PointNet<code>++</code>. We will explain the concept of point pair features in subsection <a href="#section:PPF" data-reference-type="ref" data-reference="section:PPF">3.1</a>, input-level featurization in <a href="#section:PPF_inp" data-reference-type="ref" data-reference="section:PPF_inp">3.2</a> and anchor point-based featurization in <a href="#section:PPF_anchor" data-reference-type="ref" data-reference="section:PPF_anchor">3.3</a>.</p>
<h2 data-number="3.1" id="section:PPF"><span class="header-section-number">3.1</span> Background Point Pair Features</h2>
<p>Point Pair Features (PPF), introduced in <span class="citation" data-cites="Drost2010-ppf">[11]</span>, extracts four numbers from a pair of points and their surface normal vectors, which describes the relative geometric relationship between the two points. Given two point clouds, if we can find one PPF from each point cloud respectively, we can estimate a 6 degree-of-freedom (DoF) rigid body transformation by aligning these points and normal vectors. Object pose estimation or point cloud registration can therefore be achieved by iterating over all matching pair of PPFs and conducting a Hough voting algorithm. This process can be followed by a pose clustering and refinement step to further improve the accuracy. In this project, we just take the basic PPF featurization and will briefly introduce it below. For a comprehensive introduction, please refer to <span class="citation" data-cites="Drost2010-ppf Vidal2018-rh">[11], [16]</span>.</p>
<figure>
<img src="images/ppf.png" id="fig:ppf_illu" alt="Illustration of Point Pair Feature on a point cloud. " /><figcaption aria-hidden="true">Illustration of Point Pair Feature on a point cloud. </figcaption>
</figure>
<p>As shown in Fig. <a href="#fig:ppf_illu" data-reference-type="ref" data-reference="fig:ppf_illu">1</a>, given a point cloud, we first sample <span class="math inline">\(x_1\)</span> as the reference point and then sample <span class="math inline">\(x_2\)</span> as the second point. We denote the surface normal vectors at these two points as <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, and the distance vector from <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_2\)</span> is <span class="math inline">\(d=x_2-x_1\)</span>. Point Pair Feature <span class="math inline">\(\psi_{12}\)</span> contains 4 numbers, including a distance vector and three angles. And they are defined as <span class="math display">\[\begin{aligned}
    \psi_{12} = (\left\lVert d\right\rVert_2, \; \angle (n_1, d), \; \angle(n_2, d), \; \angle (n_1, n_2)),\end{aligned}\]</span> where <span class="math inline">\(\left\lVert\cdot\right\rVert_2\)</span> denotes <span class="math inline">\(L\)</span>-2 vector norm and <span class="math inline">\(\angle(\cdot, \cdot)\)</span> computes the angle between two vectors. For numerical stability, we compute the angle as following <span class="citation" data-cites="Birdal2015-ppfbased">[13]</span> <span class="math display">\[\begin{aligned}
    \angle(v_1, v_2) = \arctan(\left\lVert v_1 \times v_2\right\rVert, v_1\cdot v_2).\end{aligned}\]</span></p>
<p>Apparently, a point pair feature describes the relative relationship between two points and is agnostic to the actual position or orientation of the point cloud. Therefore, PPF is invariant to any rigid body transformation as long as the point cloud shape does not change.</p>
<h2 data-number="3.2" id="section:PPF_inp"><span class="header-section-number">3.2</span> Global PPF Featurization</h2>
<figure>
<img src="images/pn_input.png" id="fig:pn_input" alt="PPF featurization as input to PointNet." /><figcaption aria-hidden="true">PPF featurization as input to PointNet.</figcaption>
</figure>
<p>We first tried a simple method of converting input point clouds to PPF features. We sample a single point <span class="math inline">\(r\)</span> as the reference point and then compute the 4D PPFs for all other points with respect to <span class="math inline">\(r\)</span>. As shown in Fig. <a href="#fig:pn_input" data-reference-type="ref" data-reference="fig:pn_input">2</a>, the mapping from point cloud to PPFs can be expressed as <span class="math display">\[\begin{aligned}
    M_1 : 
    \begin{bmatrix}
    r \\ x_1 \\ x_2 \\ \cdots \\ x_n
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
    \psi_{r,x_1} \\ \psi_{r,x_2} \\ \cdots \\ \psi_{r,x_n}.
    \end{bmatrix}\end{aligned}\]</span></p>
<p>And we also designed different strategies for sampling <span class="math inline">\(r\)</span>:</p>
<ul>
<li><p><em>PPF (Random)</em>: we sample a random point in the point cloud as <span class="math inline">\(r\)</span>.</p></li>
<li><p><em>PPF (Center)</em>: we first compute the mean position of the point cloud <span class="math inline">\(x_c=\sum_{i=1}^n x_I / n\)</span> and then choose the point closest to <span class="math inline">\(x_c\)</span> as <span class="math inline">\(r\)</span>, which means <span class="math inline">\(x_c = \arg\min_{x_i} \left\lVert x_c-x_i\right\rVert_2\)</span>.</p></li>
<li><p><em>PPF (Farthest)</em>: We use the point that is furthest from <span class="math inline">\(x_c\)</span> as <span class="math inline">\(r\)</span>, which means <span class="math inline">\(x_c = \arg\max_{x_i} \left\lVert x_c-x_i\right\rVert_2\)</span>.</p></li>
</ul>
<p>This method input featurization is agnostic to the network architecture and can be used for any point cloud network.</p>
<h2 data-number="3.3" id="section:PPF_anchor"><span class="header-section-number">3.3</span> Local PPF Featurization Based on Anchors</h2>
<figure>
<img src="images/pn2_input.png" id="fig:pn2_input" alt="PPF featurization based on anchor point in the first Set Abstraction layer." /><figcaption aria-hidden="true">PPF featurization based on anchor point in the first Set Abstraction layer.</figcaption>
</figure>
<p>We also incorporated the PPF featurization into the hierarchical architecture of PointNet<code>++</code> <span class="citation" data-cites="qi2017pointnet2">[2]</span>. Specifically, in the Set Abstraction layers, PointNet<code>++</code> first samples some anchors, then for each anchor, find their K nearest neighbors to form a small point cloud. Each small point cloud runs through a PointNet. In the first Set Abstraction layer, we treat the sampled anchor of each group as the reference point <span class="math inline">\(r\)</span> and compute PPF <span class="math inline">\(\psi_{r, x_i}\)</span> (<span class="math inline">\(i\in\{1,..., k\}\)</span>) for every point <span class="math inline">\(x_i\)</span> in the small group. As shown in Fig. <a href="#fig:pn2_input" data-reference-type="ref" data-reference="fig:pn2_input">3</a>, the computed PPFs are then fed into PointNet. We call this sampling strategy <em>PPF (anchor)</em>.</p>
<h1 data-number="4" id="experiment"><span class="header-section-number">4</span> Experiment</h1>
<h2 data-number="4.1" id="dataset"><span class="header-section-number">4.1</span> Dataset</h2>
<p>To evaluate our design of PPF featurization, we use the ModelNet40 <span class="citation" data-cites="Wu2015-3dshapenet">[8]</span> dataset. ModelNet40 contains 12,308 CAD models of 40 different categories, which are split into a training set of 9,840 objects and a testing set of 2,468 objects. We sub-sampled the CAD models and get 2,048 points for each point cloud, together with their surface normal vectors. We adopt the point cloud classification task, where the network is going to output a 40-dimensional probability distribution, one for each category. In the experiments below, we report the classification accuracy averaged over 40 categories.</p>
<div id="tab:my_label">
<table>
<caption>Testing class accuracy on different networks, PPF sampling strategies and training augmentation. The <code>Training w/o Augmentation</code> column shows the result of training on different PPF sampling strategies. The <code>Train w/ Augmentation</code> column shows the result of training on the augmented training point clouds and testing with <code>Original</code> and <code>Rotation</code> test sets.</caption>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2" rowspan="2">Architecture</td>
<td style="text-align: center;">Training w/o Augmentation</td>
<td style="text-align: center;" colspan="2">Training w/ Augmentation</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Rotated</code></td>
<td style="text-align: center;"><code>Original</code></td>
<td style="text-align: center;"><code>Rotated</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">PointNet <span class="citation" data-cites="Qi2017pointnet">[1]</span></td>
<td style="text-align: left;"><span class="math inline">\(Spatial\)</span></td>
<td style="text-align: center;">21.68</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(Spatial+Normal\)</span></td>
<td style="text-align: center;">22.56</td>
<td style="text-align: center;">81.50</td>
<td style="text-align: center;">77.80</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Random)\)</span></td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Center)\)</span></td>
<td style="text-align: center;"><strong>73.79</strong></td>
<td style="text-align: center;">26.07</td>
<td style="text-align: center;">25.35</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Farthest)\)</span></td>
<td style="text-align: center;">60.82</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">PointNet<code>++ </code></td>
<td style="text-align: left;"><span class="math inline">\(Spatial\texttt{++}Normal\)</span></td>
<td style="text-align: center;">28.85</td>
<td style="text-align: center;"><strong>87.01</strong></td>
<td style="text-align: center;">84.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Center)\)</span></td>
<td style="text-align: center;">53.24</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Anchor)\)</span></td>
<td style="text-align: center;"><strong>69.69</strong></td>
<td style="text-align: center;">86.28</td>
<td style="text-align: center;"><strong>86.14</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">DGCNN <span class="citation" data-cites="Wang2019-dgcnn">[3]</span></td>
<td style="text-align: left;"><span class="math inline">\(Spatial\)</span></td>
<td style="text-align: center;">26.68</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(Spatial\texttt{+}Normal\)</span></td>
<td style="text-align: center;">29.96</td>
<td style="text-align: center;">86.80</td>
<td style="text-align: center;">84.22</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Random)\)</span></td>
<td style="text-align: center;">73.08</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Center)\)</span></td>
<td style="text-align: center;"><strong>77.94</strong></td>
<td style="text-align: center;">45.51</td>
<td style="text-align: center;">46.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(PPF\ (Farthest)\)</span></td>
<td style="text-align: center;">72.60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
</div>
<h2 data-number="4.2" id="ppf-sampling-strategies"><span class="header-section-number">4.2</span> PPF Sampling Strategies</h2>
<p>We trained different PPF sampling strategies on PointNet, PointNet<code>++</code> and DGCNN on ModelNet40 dataset. To study whether the trained network can learn rotation-invariant representation, we follow the evaluation protocol proposed in <span class="citation" data-cites="taghanaki2020robustpointset">[5]</span>, and we first perform the experiments where the training data augmentation is disabled, and thus all training point clouds are from the original ModelNet40 (<em>Training Original</em>). For baseline performance, we separately trained the vanilla networks with only spatial coordinates (<em>Spatial</em>) and with spatial coordinates and surface normal vectors (<em>Spatial + Normal</em>). And for the proposed PPF featurization, we train and test using different reference point sampling strategies, including <em>PPF (Random)</em>, <em>PPF (Center)</em>, and <em>PPF (Farthest)</em>. For PointNet<code>++</code>, we also trained it on the <em>PPF (Anchor)</em> method described in Sec. <a href="#section:PPF_anchor" data-reference-type="ref" data-reference="section:PPF_anchor">3.3</a>.</p>
<p>The results show that in the absence of training data augmentation, networks with <em>Spatial</em> and <em>Spatial + Normal</em> overfit to the training set and don’t generalize to the rotated point cloud. Applying PPF significantly boosts the validation accuracy by 30<span class="math inline">\(\sim\)</span>40%. <em>PPF (Center)</em> performs the best on PointNet and DGCNN, with 73.79% and 77.94% test-time classification accuracy respectively. <em>PPF (Anchor)</em> performs the best on PointNet<code>++</code>, with 69.69% test-time classification accuracy.</p>
<figure>
<img src="images/ppf_combined.png" alt="Classification accuracy on ModelNet40 testing set on different point cloud network using different input featurizations. Note that training augmentation is not used in these experiments. " /><figcaption aria-hidden="true">Classification accuracy on ModelNet40 testing set on different point cloud network using different input featurizations. Note that training augmentation is not used in these experiments. </figcaption>
</figure>
<h2 data-number="4.3" id="training-augmentation"><span class="header-section-number">4.3</span> Training Augmentation</h2>
<p>However, in the training setup adopted by most point cloud networks, extensive augmentation on point clouds is performed and the trained networks can obtain rotation robustness to some extent. Therefore, we also perform experiments following the training data augmentation used in <span class="citation" data-cites="qi2017pointnet2">[2]</span>, where the point clouds are randomly rotated, scaled, translated, and jittered. We call this training setup <em>Training w/ Augmentation</em> in Table. <a href="#tab:my_label" data-reference-type="ref" data-reference="tab:my_label">1</a>. And we tested the trained networks on the testing point clouds with and without random rotation perturbation (<em>Original</em> and <em>Rotation</em>)</p>
<p>We can see from the results that using <em>Spatial+Normal</em> as input does lead to a performance drop when tested on <em>Rotated</em> point clouds, compared to the <em>Original</em> ones. However, PPF featurization with a single reference for the whole point cloud (<em>PPF (Center)</em>) significantly harms the testing accuracy when the training data is augmented. We hypothesize that this is because we sample a single reference point for the whole point cloud, and training data augmentation will constantly change the reference point, which confuses the network learning.</p>
<p>We tried similarly for PointNet++, except that we use <em>PPF (anchor)</em> sampling strategy rather than <em>PPF (center)</em>. We can see that applying <em>PPF (anchor)</em> on the rotated validation set improves the performance compared to not using it. We think that in the original PointNet++, the difference of spatial coordinates is taken within each group, and using PPF featurization in the local region leads to better test-time rotational robustness.</p>
<figure>
<img src="images/aug_combined.png" alt="Classification accuracy on ModelNet40 testing set on different point cloud network using different input featurizations. Note that training augmentation used in these experiments." /><figcaption aria-hidden="true">Classification accuracy on ModelNet40 testing set on different point cloud network using different input featurizations. Note that training augmentation used in these experiments.</figcaption>
</figure>
<h1 data-number="5" id="conclusion-and-future-work"><span class="header-section-number">5</span> Conclusion and Future Work</h1>
<p>In conclusion, we propose to incorporate PPF featurization into recent deep learning architectures on 3D point clouds. We design and test different strategies of sampling the reference points. The results show that PPF featurization can improve rotational robustness in the absence of training point cloud augmentation. However, sampling a single reference point globally harms the performance when training with augmentation. We also propose to convert a local point cloud to PPFs in the sampling and grouping step in PointNet++, and we find that it improves the rotational robustness in PointNet++. For future directions, we plan to incorporate PPF hierarchically into PointNet++ and test whether PPFs are robust to other point cloud perturbations.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Qi2017pointnet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">C. R. Qi, H. Su, K. Mo, and L. J. Guibas, <span>“PointNet: Deep learning on point sets for 3D classification and segmentation,”</span> 2017.</div>
</div>
<div id="ref-qi2017pointnet2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">C. R. Qi, L. Yi, H. Su, and L. J. Guibas, <span>“Pointnet++: Deep hierarchical feature learning on point sets in a metric space,”</span> in <em>NeurIPS</em>, 2017, pp. 5099–5108.</div>
</div>
<div id="ref-Wang2019-dgcnn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, <span>“Dynamic graph <span>CNN</span> for learning on point clouds,”</span> <em>arXiv:1801.07829 [cs]</em>, Jun. 2019.</div>
</div>
<div id="ref-Li2018-PointCNN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, <span>“<span>PointCNN</span>: Convolution on <span><span class="math inline">\(\mathcal\{X\}\)</span>-Transformed</span> points,”</span> <em>arXiv:1801.07791 [cs]</em>, Nov. 2018.</div>
</div>
<div id="ref-taghanaki2020robustpointset" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">S. Asgari Taghanaki, J. Luo, R. Zhang, Y. Wang, P. K. Jayaraman, and K. M. Jatavallabhula, <span>“RobustPointSet: A dataset for benchmarking robustness of point cloud classifiers,”</span> <em>arXiv preprint arXiv:2011.11572</em>, 2020.</div>
</div>
<div id="ref-Wu2019-pointconv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">W. Wu, Z. Qi, and L. Fuxin, <span>“Pointconv: Deep convolutional networks on 3d point clouds,”</span> in <em>Proceedings of the <span>IEEE/CVF</span> conference on computer vision and pattern recognition</em>, 2019, pp. 9621–9630.</div>
</div>
<div id="ref-Liu2019-densepoint" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Y. Liu, B. Fan, G. Meng, J. Lu, S. Xiang, and C. Pan, <span>“Densepoint: Learning densely contextual representation for efficient point cloud processing,”</span> in <em>Proceedings of the <span>IEEE/CVF</span> international conference on computer vision</em>, 2019, pp. 5239–5248.</div>
</div>
<div id="ref-Wu2015-3dshapenet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Z. Wu <em>et al.</em>, <span>“3d shapenets: A deep representation for volumetric shapes,”</span> in <em>Proceedings of the <span>IEEE</span> conference on computer vision and pattern recognition</em>, 2015, pp. 1912–1920.</div>
</div>
<div id="ref-Poulenard2019-sphnet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">A. Poulenard, M. Rakotosaona, Y. Ponty, and M. Ovsjanikov, <span>“Effective <span>Rotation-Invariant</span> point <span>CNN</span> with spherical harmonics kernels,”</span> in <em>2019 international conference on <span>3D</span> vision (<span>3DV</span>)</em>, Sep. 2019, pp. 47–56.</div>
</div>
<div id="ref-You2021-prin" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">Y. You <em>et al.</em>, <span>“<span>PRIN/SPRIN</span>: On extracting point-wise rotation invariant features,”</span> Feb. 2021,Available: <a href="https://arxiv.org/abs/2102.12093">https://arxiv.org/abs/2102.12093</a></div>
</div>
<div id="ref-Drost2010-ppf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">B. Drost, M. Ulrich, N. Navab, and S. Ilic, <span>“Model globally, match locally: Efficient and robust <span>3D</span> object recognition,”</span> in <em>2010 <span>IEEE</span> computer society conference on computer vision and pattern recognition</em>, 2010, pp. 998–1005.</div>
</div>
<div id="ref-Eunyoung_Kim2011-3dobject" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">Eunyoung Kim and G. Medioni, <span>“<span>3D</span> object recognition in range images using visibility context,”</span> in <em>2011 <span>IEEE/RSJ</span> international conference on intelligent robots and systems</em>, 2011, pp. 3800–3807.</div>
</div>
<div id="ref-Birdal2015-ppfbased" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">T. Birdal and S. Ilic, <span>“Point pair features based object detection and pose estimation revisited,”</span> in <em>2015 international conference on <span>3D</span> vision</em>, 2015, pp. 527–535.</div>
</div>
<div id="ref-Hinterstoisser2016-goingfurther" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">S. Hinterstoisser, V. Lepetit, N. Rajkumar, and K. Konolige, <span>“Going further with point pair features,”</span> in <em>Computer vision – <span>ECCV</span> 2016</em>, vol. 9907, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Cham: Springer International Publishing, 2016, pp. 834–848.</div>
</div>
<div id="ref-Kiforenko2018-ppfeval" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">L. Kiforenko, B. Drost, F. Tombari, N. Krüger, and A. Glent Buch, <span>“A performance evaluation of point pair features,”</span> <em>Comput. Vis. Image Underst.</em>, vol. 166, pp. 66–80, 2018.</div>
</div>
<div id="ref-Vidal2018-rh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">J. Vidal, C.-Y. Lin, X. Lladó, and R. Martı́, <span>“A method for <span>6D</span> pose estimation of <span>Free-Form</span> rigid objects using point pair features on range data,”</span> <em>Sensors</em>, vol. 18, no. 8, p. 2678, Aug. 2018.</div>
</div>
<div id="ref-Hodan2018-bop" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">T. Hodan <em>et al.</em>, <span>“<span>BOP</span>: Benchmark for <span>6D</span> object pose estimation,”</span> <em>arXiv:1808.08319 [cs]</em>, Aug. 2018.</div>
</div>
<div id="ref-Deng2018-PPFNet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">H. Deng, T. Birdal, and S. Ilic, <span>“<span>PPFNet</span>: Global context aware local features for robust <span>3D</span> point matching,”</span> in <em>2018 <span>IEEE/CVF</span> conference on computer vision and pattern recognition</em>, 2018, pp. 195–205.</div>
</div>
<div id="ref-Deng2018-PPFFoldNet" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">H. Deng, T. Birdal, and S. Ilic, <span>“<span>PPF-FoldNet</span>: Unsupervised learning of rotation invariant <span>3D</span> local descriptors,”</span> in <em>Computer vision – <span>ECCV</span> 2018</em>, vol. 11209, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds. Cham: Springer International Publishing, 2018, pp. 620–638.</div>
</div>
</div>
</body>
</html>
